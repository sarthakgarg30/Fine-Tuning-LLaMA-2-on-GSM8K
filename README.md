# Fine-Tuning-LLaMA-2-on-GSM8K
This project fine-tunes the LLaMA-2-7B model on the GSM8K dataset to enhance mathematical reasoning. It utilizes Hugging Face Transformers, PyTorch, and PEFT techniques like LoRA for efficient adaptation, optimizing the model for solving complex math problems.
